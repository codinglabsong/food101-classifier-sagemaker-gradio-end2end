# Food101 End-to-End Image Classifier (PyTorch + SageMaker + Gradio)

**A reproducible, end-to-end PyTorch pipeline for Food101 image classification.**
Supports local development, SageMaker training, flexible dataset prep, Weights & Biases, and is ready for deployment through Gradio.

## Features
- EfficientNet-B2 transfer learning (feature extraction + fine-tuning)
- Simple, reproducible training with local or SageMaker workflows
- Dataset preparation: full or small sample subsets
- Logging and experiment tracking (Weights & Biases)
- Model checkpointing and flexible configuration
- Ready for deployment (Gradio web app)

## Project Structure
```graphql
├── config/
│   └── prod.yaml              # Config for SageMaker
├── data/                      # Datasets (generated by scripts)
│   └── full/                  # Entire dataset
│   └── sample/                # Sample dataset for local testing
├── notebooks/
│   └── 00_explore.ipynb       # EDA, prototyping
├── scripts/
│   ├── download_full.py       # Download full Food101 as ImageFolder
│   ├── download_sample.py     # Create small per-class sample for local testing
│   └── remote_train.py        # Launch SageMaker training job
├── src/
│   └── train.py               # Main training script
├── .env.example               # Example for API keys/secrets
├── requirements.txt           # Pip dependencies
├── gradio_app.py              # Gradio tnterface
├── README.md
└── .gitignore
```

## Quick Start
### 1. Clone & Install
```bash
git clone https://github.com/<your-username>/food101-classifier.git
cd food101-classifier
pip install -r requirements.txt
```
*For CUDA users, see PyTorch's [install guide](https://pytorch.org/get-started/locally/)*

### 2. Prepare the Dataset
- Quick sample Food101 dataset for local development:
    ```bash
    python scripts/download_sample.py --out data/sample --train-per-class 20 --test-per-class 4
    ```
- Full Food101 dataset (may take time/disk space):
    ```bash
    python scripts/download_full.py --out data/full
    ```

### 3. Train Locally
```bash
python src/train.py \
  --batch-size 32 \
  --num-epochs-phase1 3 \
  --num-epochs-phase2 2 \
  --lr-head 1e-3 \
  --lr-backbone 1e-4 \
  --patience 3 \
  --num-workers 4 \
  --img-size 224
```
*Use `--help` for all options.*

### 4. Setting Environment Variables
Edit `.env` using `.env.example` as a guide for AWS and wandb keys.

### SageMaker Training
1. Upload tarred datasets to S3:
    ```bash
    tar czf food101-train.tar.gz -C data/full/train .
    tar czf food101-test.tar.gz -C data/full/test .
    aws s3 cp food101-train.tar.gz s3://<your-bucket>/full/
    aws s3 cp food101-test.tar.gz s3://<your-bucket>/full/
    ```

2. Launch SageMaker training:
    ```bash
    python scripts/remote_train.py
    ```

### Preprocessing Consistency & Image Size Limit

> **Important:**  
> The preprocessing pipeline (image resizing, cropping, normalization) **must be identical** between training and inference (including Gradio app or deployment).
>
> - All transforms should use parameters from `config/prod.yaml` (or your config file).
> - The value of `img_size` used for training and inference must always be ≤ 256, since images are first resized so their short edge is 256 before center cropping.  
> - **Do not set `img_size` greater than 256.** This would result in errors or ineffective cropping during inference.

**Best practice:**  
Update only your config file (not hardcoded values) when changing image size or normalization, and always reload configs in both training and inference code.

## Running the Gradio Inference App
This project includes an interactive Gradio app for making predictions with the trained model.

1. **Obtain the Trained Model:**
- Make sure you have the trained model file (`model.pth`). 
- If you trained the model yourself, it should be saved automatically to the `output/` directory.
- If you received a pre-trained model, download it and place it in the `output/` directory at the project root.

2. **Run the App Locally:**
    ```bash
    python gradio_app.py
    ```
- The app will start locally and print a link (e.g., `http://127.0.0.1:7860`) to access the web UI in your browser.

## Requirements
- See `requirements.txt`
- Python ≥ 3.8
- PyTorch >= 2.2

## Contributing
Open to issues and pull requests!

## References
- [Food-101 Dataset](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)
- [EfficientNet Paper](https://arxiv.org/abs/1905.11946)
- [PyTorch Docs](https://pytorch.org/)

## License
This project is licensed under the MIT License.

## Tips:
- .env.example helps keep secrets out of git.
- .gitignore: Don't track datasets, outputs, or .env.